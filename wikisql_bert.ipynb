{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0404 16:59:31.571887 139964001732352 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019-present NAVER Corp.\n",
    "# Apache License v2.0\n",
    "\n",
    "# Wonseok Hwang\n",
    "# Sep30, 2018\n",
    "import os, sys, argparse, re, json,ujson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pylab import *\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "# import torchvision.datasets as dsets\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sqlova.utils.utils_wikisql import *\n",
    "from sqlova.model.nl2sql.wikisql_models import *\n",
    "from sqlnet.dbengine import DBEngine\n",
    "import bert.tokenization as tokenization\n",
    "from bert.modeling import BertConfig, BertModel\n",
    "from tensorflow.keras import backend as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_CPU = 1\n",
    "num_GPU = 0\n",
    "num_cores = 8\n",
    "config = tf.ConfigProto(intra_op_parallelism_threads=num_cores,\n",
    "                    inter_op_parallelism_threads=num_cores,\n",
    "                    allow_soft_placement=True,\n",
    "                    device_count = {'CPU' : num_CPU,\n",
    "                                    'GPU' : num_GPU}\n",
    "                   )\n",
    "\n",
    "sess = tf.Session(config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFHUB_CACHE_DIR'] = '/DataDrive/master-wikisql/tfhub_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{-1.0, 0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(wc.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_num_headers = len(set(wc.flatten()))\n",
    "w = np.ones([wc.shape[0],wc.shape[1],max_num_headers])\n",
    "w.shape\n",
    "w[np.where(wc==-1)] = 0\n",
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, train_labels = makeTrainingData(use_reduced_set=True,sample_size=2000)\n",
    "sc,sa,wn,wc,wo = train_labels\n",
    "max_num_headers = len(set(wc.flatten()))\n",
    "w = np.ones([wc.shape[0],wc.shape[1]])\n",
    "#w[np.where(w>=0)] = 1\n",
    "w[np.where(wc==-1)] = 0\n",
    "max_out_len = wc.shape[1]\n",
    "split = int(0.8*len(wc))\n",
    "test_data = training_data[split:]\n",
    "training_data = training_data[:split]\n",
    "\n",
    "test_labels_sc = sc[split:]\n",
    "train_labels_sc = sc[:split]\n",
    "\n",
    "test_labels_sa = sa[split:]\n",
    "train_labels_sa = sa[:split]\n",
    "\n",
    "\n",
    "test_labels_wn = wn[split:]\n",
    "train_labels_wn = wn[:split]\n",
    "\n",
    "test_labels_wc = wc[split:]\n",
    "train_labels_wc = wc[:split]\n",
    "\n",
    "test_labels_wo = wo[split:]\n",
    "train_labels_wo = wo[:split]\n",
    "\n",
    "train_weights = w[:split]\n",
    "test_weights = w[split:]\n",
    "#test_labels = np.array(test_labels).reshape(-1, 1)\n",
    "#train_labels= np.array(train_labels).reshape(-1, 1)\n",
    "\n",
    "train_input_ids,train_input_masks,train_segment_ids = formatForBert(training_data)\n",
    "test_input_ids,test_input_masks,test_segment_ids = formatForBert(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 3)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_wc.shape\n",
    "test_labels_wo.shape\n",
    "train_labels_wo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,train_labels = makeTrainingData(use_reduced_set=True,sample_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0404 17:06:16.530830 139964001732352 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0404 17:06:18.430361 139964001732352 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_4 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "denseSC (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseSA (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWN (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWC (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWO (Dense)                 (None, 256)          196864      bert_layer_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sc_output (Dense)               (None, 13)           3341        denseSC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sa_output (Dense)               (None, 4)            1028        denseSA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wn_output (Dense)               (None, 4)            1028        denseWN[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wc_output (Dense)               (None, 13)           3341        denseWC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wo_output (Dense)               (None, 3)            771         denseWO[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,098,719\n",
      "Trainable params: 3,943,717\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "WARNING:tensorflow:From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0404 17:06:26.281414 139964001732352 deprecation.py:323] From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 352/1600 [=====>........................] - ETA: 9:21 - loss: 6.8084 - sc_output_loss: 2.3744 - sa_output_loss: 1.0673 - wn_output_loss: 0.6153 - wc_output_loss: 2.4631 - wo_output_loss: 0.2884 - sc_output_acc: 0.1278 - sa_output_acc: 0.6335 - wn_output_acc: 0.8011 - wc_output_acc: 0.2159 - wo_output_acc: 0.9062"
     ]
    }
   ],
   "source": [
    "\n",
    "# Build the rest of the classifier \n",
    "    \n",
    "max_out_len = 3\n",
    "model = build_model(max_seq_length=222,max_out_len=max_out_len)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    #[train_input_ids, train_input_masks, train_segment_ids, train_weights], \n",
    "    [train_input_ids, train_input_masks, train_segment_ids], \n",
    "        {\"sc_output\":train_labels_sc,\"sa_output\":train_labels_sa,\n",
    "         \"wn_output\":train_labels_wn,\"wc_output\":train_labels_wc,\"wo_output\":train_labels_wo},\n",
    "    validation_data=(#[test_input_ids,test_input_masks,test_segment_ids, test_weights],\n",
    "                [test_input_ids,test_input_masks,test_segment_ids],\n",
    "                     {\"sc_output\":test_labels_sc,\"sa_output\":test_labels_sa,\n",
    "                      \"wn_output\":test_labels_wn,\"wc_output\":test_labels_wc,\n",
    "                     \"wo_output\":test_labels_wo}),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "test_example = test_data[i]\n",
    "p_id,p_mask,p_seg = formatForBert([test_example])\n",
    "\n",
    "input1,input2,input3,input4 = p_id,p_mask,p_seg,np.array([test_weights[i,:]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': ['how',\n",
       "  'many',\n",
       "  'end',\n",
       "  'of',\n",
       "  'term',\n",
       "  'with',\n",
       "  'age',\n",
       "  'at',\n",
       "  'inauguration',\n",
       "  'being',\n",
       "  '64-066',\n",
       "  '64years',\n",
       "  ',',\n",
       "  '66days'],\n",
       " 'header': ['#',\n",
       "  'president',\n",
       "  'date of birth',\n",
       "  'date of inauguration',\n",
       "  'age at inauguration',\n",
       "  'end of term',\n",
       "  'length of retirement',\n",
       "  'date of death',\n",
       "  'lifespan']}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0.2668911 , 0.1030051 , 0.10034642, 0.14030127, 0.17761466,\n",
       "         0.08895458, 0.05044549, 0.02228669, 0.03487641, 0.00805316,\n",
       "         0.0041892 , 0.00161389, 0.00142196]], dtype=float32),\n",
       " array([[0.82275784, 0.02035269, 0.05989522, 0.09699427]], dtype=float32),\n",
       " array([[0.0288064 , 0.89778435, 0.07083937, 0.00256985]], dtype=float32),\n",
       " array([[0.94555503, 0.15660135, 0.00410538]], dtype=float32),\n",
       " array([[0.95385194, 0.9670656 , 0.01036781]], dtype=float32)]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([input1,input2,input3,input4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"sc_output\":train_labels_sc,\"sa_output\":train_labels_sa,\n",
    " \"wn_output\":train_labels_wn,\"wc_output\":train_labels_wc,\"wo_output\":train_labels_wo},\n",
    "train_labels_wo.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    #weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(max_seq_length,max_out_len,max_header_length=13): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    #weight_input = tf.keras.layers.Input(shape=(max_out_len,), name=\"ce_weights\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    all_inputs = [in_id, in_mask, in_segment]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "\n",
    "    denseSC = tf.keras.layers.Dense(256, activation='relu',name=\"denseSC\")(bert_output)\n",
    "    denseSA = tf.keras.layers.Dense(256, activation='relu',name=\"denseSA\")(bert_output)\n",
    "    denseWN = tf.keras.layers.Dense(256, activation='relu',name=\"denseWN\")(bert_output)\n",
    "    denseWC = tf.keras.layers.Dense(256, activation='relu',name=\"denseWC\")(bert_output)\n",
    "    denseWO = tf.keras.layers.Dense(256, activation='relu',name=\"denseWO\")(bert_output)\n",
    "    \n",
    "    \n",
    "    sc = tf.keras.layers.Dense(max_header_length, activation='softmax',name=\"sc_output\")(denseSC)\n",
    "    sa = tf.keras.layers.Dense(4, activation='softmax',name=\"sa_output\")(denseSA)\n",
    "    wn = tf.keras.layers.Dense(4, activation='softmax',name=\"wn_output\")(denseWN)    \n",
    "    wc = tf.keras.layers.Dense(max_header_length, activation='softmax',name=\"wc_output\")(denseWC)\n",
    "    wo = tf.keras.layers.Dense(3, activation='softmax',name=\"wo_output\")(denseWO)\n",
    "\n",
    "    #loss = weighted_categorical_crossentropy(weight_input)    \n",
    "    #loss=,\n",
    "    model = tf.keras.models.Model(inputs=all_inputs, outputs=[sc,sa,wn,wc,wo])\n",
    "    \n",
    "\n",
    "    model.compile(loss={\"sc_output\":\"sparse_categorical_crossentropy\",\n",
    "                        \"sa_output\":\"sparse_categorical_crossentropy\",\n",
    "                        \"wn_output\":\"sparse_categorical_crossentropy\",\n",
    "                        #'wc_output': loss, \n",
    "                        #'wo_output': loss}, optimizer='adam', metrics=['accuracy'])\n",
    "                        'wc_output': 'categorical_crossentropy', \n",
    "                        'wo_output': 'categorical_crossentropy'}, optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 13)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_labels_wc.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE ACTUAL BERT LAYER\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0403 12:35:06.081140 139717726570240 deprecation.py:323] From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_hub_module_handle):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        tf.print('GRAPH STARTED')\n",
    "        bert_module = hub.Module(bert_hub_module_handle)\n",
    "        tf.print('MODULE DOWNLOADED')\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        tf.print('TOKENIZATION INFO LOADED')\n",
    "        with tf.Session() as sess:\n",
    "            tf.print('SESSION STARTED')\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                tokenization_info[\"do_lower_case\"]])\n",
    "            tf.print('VARIABLES RAN')\n",
    "            return FullTokenizer(\n",
    "                vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT FORMATTING AND OUTPUT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(tokenizer, nlu1_tok, hds1):\n",
    "   tokens = []\n",
    "   segment_ids = []\n",
    "   tokens.append(\"[CLS]\")\n",
    "\n",
    "   segment_ids.append(0)\n",
    "   for token in nlu1_tok:\n",
    "       tokens.append(token)\n",
    "       segment_ids.append(0)\n",
    "   tokens.append(\"[SEP]\")\n",
    "   segment_ids.append(0)\n",
    "\n",
    "   header_break = [len(tokens),len(tokens) + len(hds1)]\n",
    "   # for doc\n",
    "   for i, hds11 in enumerate(hds1):\n",
    "       sub_tok = tokenizer.tokenize(hds11)\n",
    "       tokens += sub_tok\n",
    "       segment_ids += [1] * len(sub_tok)\n",
    "       if i < len(hds1)-1:\n",
    "           tokens.append(\"[SEP]\")\n",
    "           segment_ids.append(0)\n",
    "       elif i == len(hds1)-1:\n",
    "           tokens.append(\"[SEP]\")\n",
    "           segment_ids.append(1)\n",
    "       else:\n",
    "           raise EnvironmentError\n",
    "\n",
    "\n",
    "   return tokens, segment_ids, header_break\n",
    "\n",
    "def _formatForBert(tokenizer, query_token, header_token, max_seq_length):\n",
    "    #####For each example, tokenize with BERT tokenizer\n",
    "    #Mark each example begining, and separate each header \n",
    "    #so BERT understands the query is a meaningful sequence of words but the headers are not\n",
    "    #concatenate the tokens with breaks but preserve where the headers stop and start\n",
    "    #then pad each example to the max sequence length within each batch\n",
    "    double_tokenized_tokens = []\n",
    "    for (i, token) in enumerate(query_token):\n",
    "            #t_to_tt_idx1.append(\n",
    "            #    len(double_tokenized_tokens))  # all_doc_tokens[ indicate the start position of original 'white-space' tokens.\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "            #    tt_to_t_idx1.append(i)\n",
    "                double_tokenized_tokens.append(sub_token)  # all_doc_tokens are further tokenized using WordPiece tokenizer\n",
    "\n",
    "    tokens1, segment_ids1, header_break = generate_inputs(tokenizer, double_tokenized_tokens, header_token)\n",
    "    input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "    # Input masks\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask1 = [1] * len(input_ids1)\n",
    "\n",
    "        # 3. Zero-pad up to the sequence length.\n",
    "    while len(input_ids1) < max_seq_length:\n",
    "            input_ids1.append(0)\n",
    "            input_mask1.append(0)\n",
    "            segment_ids1.append(0)\n",
    "\n",
    "    assert len(input_ids1) == max_seq_length\n",
    "    assert len(input_mask1) == max_seq_length\n",
    "    assert len(segment_ids1) == max_seq_length\n",
    "    return input_ids1,tokens1,segment_ids1,input_mask1,header_break\n",
    "\n",
    "def formatForBert(train_data,BERT_PT_PATH= \"/DataDrive/master-wikisql/annotated_data/\",bert_type=\"uncased_L-12_H-768_A-12\",max_seq_length=222):\n",
    "    \n",
    "    input_ids = []\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    header_breaks = []\n",
    "\n",
    "    bert_config_file = os.path.join(BERT_PT_PATH, f'bert_config_{bert_type}.json')\n",
    "    vocab_file = os.path.join(BERT_PT_PATH, f'vocab_{bert_type}.txt')\n",
    "\n",
    "    bert_config = BertConfig.from_json_file(bert_config_file)\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=True)\n",
    "    \n",
    "    for train_example in train_data:\n",
    "        query_token = train_example[\"query\"]\n",
    "        header_token = train_example[\"header\"]\n",
    "        input_ids1, tokens1, segment_ids1, input_mask1, header_break = \\\n",
    "                _formatForBert(tokenizer, query_token,header_token,max_seq_length)\n",
    "        input_ids.append(input_ids1)\n",
    "        tokens.append(tokens1)\n",
    "        segment_ids.append(segment_ids1)\n",
    "        input_masks.append(input_mask1)\n",
    "        header_breaks.append(header_break)\n",
    "\n",
    "    #TODO return header breaks and use it too\n",
    "    #bert_inputs = []\n",
    "    return np.array(input_ids), np.array(input_masks), np.array(segment_ids)\n",
    "\n",
    "def getBertOutput(bert_inputs, max_seq_length):\n",
    "    all_input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name = \"input_ids\")\n",
    "    all_input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name = \"input_masks\")\n",
    "    all_segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), name = \"segment_ids\")\n",
    "    bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING INPUT, PARSING INTO WHAT WILL BE FED TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTrainingData(wikisql_path = \"/DataDrive/master-wikisql/annotated_data/\",sample_size=32,use_reduced_set=False):\n",
    "    train_data, train_table, dev_data, dev_table, _, _ = load_wikisql(wikisql_path,\n",
    "                                                                  use_reduced_set, sample_size, no_w2i=True, no_hs_tok=True)\n",
    "    \n",
    "    return parseToTextLines(train_data,train_table)\n",
    "\n",
    "def parseToTextLines(train_data,train_table):\n",
    "    train_dataframe = pd.DataFrame(train_data)\n",
    "    question_toks = [[x.lower() for x in sublist] for sublist in train_dataframe[\"question_tok\"].tolist()]\n",
    "    sql = train_dataframe[\"sql\"].tolist()\n",
    "    table_ids = train_dataframe[\"table_id\"].tolist()\n",
    "    table_headers = [[x.lower() for x in train_table[tid][\"header\"]] for tid in table_ids]\n",
    "    labels = parseSqlToLabels(sql)\n",
    "    assert len(question_toks)==len(table_headers)\n",
    "    \n",
    "    keras_model_train_data = [{\"query\":question_toks[i],\"header\":table_headers[i]} for i in range(len(question_toks))]\n",
    "    return keras_model_train_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikisql(path_wikisql, toy_model, toy_size, bert=False, no_w2i=False, no_hs_tok=False, aug=False):\n",
    "    # Get data\n",
    "    train_data, train_table = load_wikisql_data(path_wikisql, mode='train', toy_model=toy_model, toy_size=toy_size, no_hs_tok=no_hs_tok, aug=aug)\n",
    "    dev_data, dev_table = load_wikisql_data(path_wikisql, mode='dev', toy_model=toy_model, toy_size=toy_size, no_hs_tok=no_hs_tok)\n",
    "\n",
    "\n",
    "    # Get word vector\n",
    "    if no_w2i:\n",
    "        w2i, wemb = None, None\n",
    "    else:\n",
    "        w2i, wemb = load_w2i_wemb(path_wikisql, bert)\n",
    "\n",
    "\n",
    "    return train_data, train_table, dev_data, dev_table, w2i, wemb\n",
    "\n",
    "\n",
    "def load_wikisql_data(path_wikisql, mode='train', toy_model=False, toy_size=10, no_hs_tok=False, aug=False):\n",
    "    \"\"\" Load training sets\n",
    "    \"\"\"\n",
    "    if aug:\n",
    "        mode = f\"aug.{mode}\"\n",
    "        print('Augmented data is loaded!')\n",
    "\n",
    "    path_sql = os.path.join(path_wikisql, mode+'_tok.jsonl')\n",
    "    if no_hs_tok:\n",
    "        path_table = os.path.join(path_wikisql, mode + '.tables.jsonl')\n",
    "    else:\n",
    "        path_table = os.path.join(path_wikisql, mode+'_tok.tables.jsonl')\n",
    "\n",
    "    data = []\n",
    "    table = {}\n",
    "    with open(path_sql) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if toy_model and idx >= toy_size:\n",
    "                break\n",
    "\n",
    "            t1 = json.loads(line.strip())\n",
    "            data.append(t1)\n",
    "\n",
    "    with open(path_table) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if toy_model and idx > toy_size:\n",
    "                break\n",
    "\n",
    "            t1 = json.loads(line.strip())\n",
    "            table[t1['id']] = t1\n",
    "\n",
    "    return data, table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMATTING SQL LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g(sql_i):\n",
    "    \"\"\" for backward compatibility, separated with get_g\"\"\"\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, psql_i1 in enumerate(sql_i):\n",
    "        g_sc.append( psql_i1[\"sel\"] )\n",
    "        g_sa.append( psql_i1[\"agg\"])\n",
    "\n",
    "        conds = psql_i1['conds']\n",
    "        if not psql_i1[\"agg\"] < 0:\n",
    "            g_wn.append( len( conds ) )\n",
    "            g_wc.append( get_wc1(conds) )\n",
    "            g_wo.append( get_wo1(conds) )\n",
    "            g_wv.append( get_wv1(conds) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv\n",
    "\n",
    "def padWhereConditions(clause_list):\n",
    "    b = np.zeros([len(clause_list),len(max(clause_list,key = lambda x: len(x)))])\n",
    "    b[:] = -1\n",
    "    for i,j in enumerate(clause_list):\n",
    "        b[i][0:len(j)] = j\n",
    "    return(b)\n",
    "def multihotEncodeWhereThings(where_things):\n",
    "    max_where_things = len(set([x for y in where_things for x in y]))\n",
    "    where_things_multihot = np.zeros([len(where_things),max_where_things])\n",
    "    for i, where_things_locs in enumerate(where_things):\n",
    "        where_things_multihot[i][where_things_locs] = 1\n",
    "        \n",
    "    return where_things_multihot\n",
    "    \n",
    "def parseSqlToLabels(sql): \n",
    "    #return sql\n",
    "    #return [random.randint(0, 1) for x in sql]\n",
    "    sc,sa,wn,wc,wo,wv = get_g(sql)\n",
    "    wc,wo = multihotEncodeWhereThings(wc),multihotEncodeWhereThings(wo)\n",
    "    return np.array(sc),np.array(sa),np.array(wn),wc,wo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, train_labels = makeTrainingData(use_reduced_set=True,sample_size=2000)\n",
    "sc,sa,wn,wc,wo = train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 13), (2000, 3))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wc.shape,wo.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
