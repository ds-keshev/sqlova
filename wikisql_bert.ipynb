{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0404 14:14:03.767019 140244744500992 __init__.py:56] Some hub symbols are not available because TensorFlow version is less than 1.14\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2019-present NAVER Corp.\n",
    "# Apache License v2.0\n",
    "\n",
    "# Wonseok Hwang\n",
    "# Sep30, 2018\n",
    "import os, sys, argparse, re, json,ujson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.pylab import *\n",
    "import tensorflow as tf\n",
    "import random as python_random\n",
    "# import torchvision.datasets as dsets\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from sqlova.utils.utils_wikisql import *\n",
    "from sqlova.model.nl2sql.wikisql_models import *\n",
    "from sqlnet.dbengine import DBEngine\n",
    "import bert.tokenization as tokenization\n",
    "from bert.modeling import BertConfig, BertModel\n",
    "from tensorflow.keras import backend as K\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TFHUB_CACHE_DIR'] = '/DataDrive/master-wikisql/tfhub_cache/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_path = \"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, train_labels = makeTrainingData(use_reduced_set=True,sample_size=2000)\n",
    "sc,sa,wn,wc,wo = train_labels\n",
    "w = np.ones_like(wc.copy())\n",
    "#w[np.where(w>=0)] = 1\n",
    "w[np.where(wc==-1)] = 0\n",
    "max_out_len = wc.shape[1]\n",
    "split = int(0.8*len(wc))\n",
    "test_data = training_data[split:]\n",
    "training_data = training_data[:split]\n",
    "\n",
    "test_labels_sc = sc[split:]\n",
    "train_labels_sc = sc[:split]\n",
    "\n",
    "test_labels_sa = sa[split:]\n",
    "train_labels_sa = sa[:split]\n",
    "\n",
    "\n",
    "test_labels_wn = wn[split:]\n",
    "train_labels_wn = wn[:split]\n",
    "\n",
    "test_labels_wc = wc[split:]\n",
    "train_labels_wc = wc[:split]\n",
    "\n",
    "test_labels_wo = wo[split:]\n",
    "train_labels_wo = wo[:split]\n",
    "\n",
    "train_weights = w[:split]\n",
    "test_weights = w[split:]\n",
    "#test_labels = np.array(test_labels).reshape(-1, 1)\n",
    "#train_labels= np.array(train_labels).reshape(-1, 1)\n",
    "\n",
    "train_input_ids,train_input_masks,train_segment_ids = formatForBert(training_data)\n",
    "test_input_ids,test_input_masks,test_segment_ids = formatForBert(test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data,train_labels = makeTrainingData(use_reduced_set=True,sample_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0404 14:14:49.037832 140244744500992 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0404 14:14:49.766262 140244744500992 deprecation.py:323] From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0404 14:14:50.694621 140244744500992 saver.py:1483] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_ids (InputLayer)          (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_masks (InputLayer)        (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        (None, 222)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bert_layer_1 (BertLayer)        (None, 768)          110104890   input_ids[0][0]                  \n",
      "                                                                 input_masks[0][0]                \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "denseSC (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseSA (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWN (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWC (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "denseWO (Dense)                 (None, 256)          196864      bert_layer_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "sc_output (Dense)               (None, 13)           3341        denseSC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "sa_output (Dense)               (None, 4)            1028        denseSA[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wn_output (Dense)               (None, 4)            1028        denseWN[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wc_output (Dense)               (None, 3)            771         denseWC[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "wo_output (Dense)               (None, 3)            771         denseWO[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 111,096,149\n",
      "Trainable params: 3,941,147\n",
      "Non-trainable params: 107,155,002\n",
      "__________________________________________________________________________________________________\n",
      "Train on 1600 samples, validate on 400 samples\n",
      "WARNING:tensorflow:From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0404 14:14:52.946391 140244744500992 deprecation.py:323] From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1600/1600 [==============================] - 700s 437ms/sample - loss: 4.5674 - sc_output_loss: 2.2422 - sa_output_loss: 1.0023 - wn_output_loss: 0.4591 - wc_output_loss: 0.8552 - wo_output_loss: 0.0086 - sc_output_acc: 0.1513 - sa_output_acc: 0.7019 - wn_output_acc: 0.8919 - wc_output_acc: 0.9688 - wo_output_acc: 0.6463 - val_loss: 4.3368 - val_sc_output_loss: 2.2905 - val_sa_output_loss: 0.9130 - val_wn_output_loss: 0.3565 - val_wc_output_loss: 0.7750 - val_wo_output_loss: 0.0018 - val_sc_output_acc: 0.1000 - val_sa_output_acc: 0.7125 - val_wn_output_acc: 0.9050 - val_wc_output_acc: 0.9750 - val_wo_output_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8cec597898>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Build the rest of the classifier \n",
    "    \n",
    "\n",
    "model = build_model(max_seq_length=222,max_out_len=max_out_len)\n",
    "\n",
    "# Instantiate variables\n",
    "initialize_vars(sess)\n",
    "\n",
    "model.fit(\n",
    "    [train_input_ids, train_input_masks, train_segment_ids, train_weights], \n",
    "        {\"sc_output\":train_labels_sc,\"sa_output\":train_labels_sa,\n",
    "         \"wn_output\":train_labels_wn,\"wc_output\":train_labels_wc,\"wo_output\":train_labels_wo},\n",
    "    validation_data=([test_input_ids,test_input_masks,test_segment_ids, test_weights],\n",
    "                     {\"sc_output\":test_labels_sc,\"sa_output\":test_labels_sa,\n",
    "                      \"wn_output\":test_labels_wn,\"wc_output\":test_labels_wc,\n",
    "                     \"wo_output\":test_labels_wo}),\n",
    "    epochs=1,\n",
    "    batch_size=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 1\n",
    "test_example = test_data[i]\n",
    "input1,input2,input3,input4 = test_input_ids,test_input_masks,test_segment_ids[i], test_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking input: expected input_ids to have shape (222,) but got array with shape (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b8f18e0839f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minput1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1094\u001b[0m       \u001b[0;31m# batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m       x, _, _ = self._standardize_user_data(\n\u001b[0;32m-> 1096\u001b[0;31m           x, check_steps=True, steps_name='steps', steps=steps)\n\u001b[0m\u001b[1;32m   1097\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m     if (self.run_eagerly or (isinstance(x, iterator_ops.EagerIterator) and\n",
      "\u001b[0;32m/DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle)\u001b[0m\n\u001b[1;32m   2380\u001b[0m         \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2381\u001b[0m         \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2382\u001b[0;31m         exception_prefix='input')\n\u001b[0m\u001b[1;32m   2383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2384\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    360\u001b[0m                 \u001b[0;34m'Error when checking '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mexception_prefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m                 ' but got array with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    363\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking input: expected input_ids to have shape (222,) but got array with shape (1,)"
     ]
    }
   ],
   "source": [
    "model.predict([input1,input2,input3,input4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\"sc_output\":train_labels_sc,\"sa_output\":train_labels_sa,\n",
    " \"wn_output\":train_labels_wn,\"wc_output\":train_labels_wc,\"wo_output\":train_labels_wo},\n",
    "train_labels_wo.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "def weighted_categorical_crossentropy(weights):\n",
    "    \"\"\"\n",
    "    A weighted version of keras.objectives.categorical_crossentropy\n",
    "    \n",
    "    Variables:\n",
    "        weights: numpy array of shape (C,) where C is the number of classes\n",
    "    \n",
    "    Usage:\n",
    "        weights = np.array([0.5,2,10]) # Class one at 0.5, class 2 twice the normal weights, class 3 10x.\n",
    "        loss = weighted_categorical_crossentropy(weights)\n",
    "        model.compile(loss=loss,optimizer='adam')\n",
    "    \"\"\"\n",
    "    \n",
    "    #weights = K.variable(weights)\n",
    "        \n",
    "    def loss(y_true, y_pred):\n",
    "        # scale predictions so that the class probas of each sample sum to 1\n",
    "        y_pred /= K.sum(y_pred, axis=-1, keepdims=True)\n",
    "        # clip to prevent NaN's and Inf's\n",
    "        y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "        # calc\n",
    "        loss = y_true * K.log(y_pred) * weights\n",
    "        loss = -K.sum(loss, -1)\n",
    "        return loss\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def build_model(max_seq_length,max_out_len): \n",
    "    in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "    in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "    in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "    weight_input = tf.keras.layers.Input(shape=(max_out_len,), name=\"ce_weights\")\n",
    "    \n",
    "    bert_inputs = [in_id, in_mask, in_segment]\n",
    "    all_inputs = [in_id, in_mask, in_segment, weight_input]\n",
    "    bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "\n",
    "    denseSC = tf.keras.layers.Dense(256, activation='relu',name=\"denseSC\")(bert_output)\n",
    "    denseSA = tf.keras.layers.Dense(256, activation='relu',name=\"denseSA\")(bert_output)\n",
    "    denseWN = tf.keras.layers.Dense(256, activation='relu',name=\"denseWN\")(bert_output)\n",
    "    denseWC = tf.keras.layers.Dense(256, activation='relu',name=\"denseWC\")(bert_output)\n",
    "    denseWO = tf.keras.layers.Dense(256, activation='relu',name=\"denseWO\")(bert_output)\n",
    "    \n",
    "    \n",
    "    sc = tf.keras.layers.Dense(13, activation='softmax',name=\"sc_output\")(denseSC)\n",
    "    sa = tf.keras.layers.Dense(4, activation='softmax',name=\"sa_output\")(denseSA)\n",
    "    wn = tf.keras.layers.Dense(4, activation='softmax',name=\"wn_output\")(denseWN)    \n",
    "    wc = tf.keras.layers.Dense(max_out_len, activation='sigmoid',name=\"wc_output\")(denseWC)\n",
    "    wo = tf.keras.layers.Dense(max_out_len, activation='sigmoid',name=\"wo_output\")(denseWO)\n",
    "\n",
    "    loss = weighted_categorical_crossentropy(weight_input)    \n",
    "    #loss=,\n",
    "    model = tf.keras.models.Model(inputs=all_inputs, outputs=[sc,sa,wn,wc,wo])\n",
    "    \n",
    "\n",
    "    model.compile(loss={\"sc_output\":\"sparse_categorical_crossentropy\",\n",
    "                        \"sa_output\":\"sparse_categorical_crossentropy\",\n",
    "                        \"wn_output\":\"sparse_categorical_crossentropy\",\n",
    "                        'wc_output': loss, \n",
    "                        'wo_output': loss}, optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def initialize_vars(sess):\n",
    "    sess.run(tf.local_variables_initializer())\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "    K.set_session(sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'max_out_len' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-f0d49ec1664b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0min_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"input_masks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0min_segment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_seq_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"segment_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mweight_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_out_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ce_weights\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbert_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0min_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_segment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'max_out_len' is not defined"
     ]
    }
   ],
   "source": [
    "max_seq_length = 222\n",
    "\n",
    "in_id = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_ids\")\n",
    "in_mask = tf.keras.layers.Input(shape=(max_seq_length,), name=\"input_masks\")\n",
    "in_segment = tf.keras.layers.Input(shape=(max_seq_length,), name=\"segment_ids\")\n",
    "weight_input = tf.keras.layers.Input(shape=(max_out_len,), name=\"ce_weights\")\n",
    "\n",
    "bert_inputs = [in_id, in_mask, in_segment]\n",
    "all_inputs = [in_id, in_mask, in_segment, weight_input]\n",
    "bert_output = BertLayer(n_fine_tune_layers=3)(bert_inputs)\n",
    "\n",
    "denseSC = tf.keras.layers.Dense(256, activation='relu',name=\"denseSC\")(bert_output)\n",
    "denseSA = tf.keras.layers.Dense(256, activation='relu',name=\"denseSA\")(bert_output)\n",
    "denseWN = tf.keras.layers.Dense(256, activation='relu',name=\"denseWN\")(bert_output)\n",
    "denseWC = tf.keras.layers.Dense(256, activation='relu',name=\"denseWC\")(bert_output)\n",
    "denseWO = tf.keras.layers.Dense(256, activation='relu',name=\"denseWO\")(bert_output)\n",
    "\n",
    "\n",
    "sc = tf.keras.layers.Dense(13, activation='softmax',name=\"sc_output\")(denseSC)\n",
    "sa = tf.keras.layers.Dense(4, activation='softmax',name=\"sa_output\")(denseSA)\n",
    "wn = tf.keras.layers.Dense(4, activation='softmax',name=\"wn_output\")(denseWN)    \n",
    "wc = tf.keras.layers.Dense(max_out_len, activation='sigmoid',name=\"wc_output\")(denseWC)\n",
    "wo = tf.keras.layers.Dense(max_out_len, activation='sigmoid',name=\"wo_output\")(denseWO)\n",
    "wo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DEFINE THE ACTUAL BERT LAYER\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertLayer(tf.layers.Layer):\n",
    "    def __init__(self, n_fine_tune_layers=10, **kwargs):\n",
    "        self.n_fine_tune_layers = n_fine_tune_layers\n",
    "        self.trainable = True\n",
    "        self.output_size = 768\n",
    "        super(BertLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.bert = hub.Module(\n",
    "            bert_path,\n",
    "            trainable=self.trainable,\n",
    "            name=\"{}_module\".format(self.name)\n",
    "        )\n",
    "        trainable_vars = self.bert.variables\n",
    "        \n",
    "        # Remove unused layers\n",
    "        trainable_vars = [var for var in trainable_vars if not \"/cls/\" in var.name]\n",
    "        \n",
    "        # Select how many layers to fine tune\n",
    "        trainable_vars = trainable_vars[-self.n_fine_tune_layers :]\n",
    "        \n",
    "        # Add to trainable weights\n",
    "        for var in trainable_vars:\n",
    "            self._trainable_weights.append(var)\n",
    "        \n",
    "        # Add non-trainable weights\n",
    "        for var in self.bert.variables:\n",
    "            if var not in self._trainable_weights:\n",
    "                self._non_trainable_weights.append(var)\n",
    "        \n",
    "        super(BertLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        inputs = [K.cast(x, dtype=\"int32\") for x in inputs]\n",
    "        input_ids, input_mask, segment_ids = inputs\n",
    "        bert_inputs = dict(\n",
    "            input_ids=input_ids, input_mask=input_mask, segment_ids=segment_ids\n",
    "        )\n",
    "        result = self.bert(inputs=bert_inputs, signature=\"tokens\", as_dict=True)[\n",
    "            \"pooled_output\"\n",
    "        ]\n",
    "        return result\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0403 12:35:06.081140 139717726570240 deprecation.py:323] From /DataDrive/master-wikisql/wsqlenv/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py:3632: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "bert_module = hub.Module(\"https://tfhub.dev/google/bert_uncased_L-12_H-768_A-12/1\", trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer_from_hub_module(bert_hub_module_handle):\n",
    "    \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n",
    "    with tf.Graph().as_default():\n",
    "        tf.print('GRAPH STARTED')\n",
    "        bert_module = hub.Module(bert_hub_module_handle)\n",
    "        tf.print('MODULE DOWNLOADED')\n",
    "        tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n",
    "        tf.print('TOKENIZATION INFO LOADED')\n",
    "        with tf.Session() as sess:\n",
    "            tf.print('SESSION STARTED')\n",
    "            vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n",
    "                                                tokenization_info[\"do_lower_case\"]])\n",
    "            tf.print('VARIABLES RAN')\n",
    "            return FullTokenizer(\n",
    "                vocab_file=vocab_file, do_lower_case=do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT FORMATTING AND OUTPUT GENERATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_inputs(tokenizer, nlu1_tok, hds1):\n",
    "   tokens = []\n",
    "   segment_ids = []\n",
    "   tokens.append(\"[CLS]\")\n",
    "\n",
    "   segment_ids.append(0)\n",
    "   for token in nlu1_tok:\n",
    "       tokens.append(token)\n",
    "       segment_ids.append(0)\n",
    "   tokens.append(\"[SEP]\")\n",
    "   segment_ids.append(0)\n",
    "\n",
    "   header_break = [len(tokens),len(tokens) + len(hds1)]\n",
    "   # for doc\n",
    "   for i, hds11 in enumerate(hds1):\n",
    "       sub_tok = tokenizer.tokenize(hds11)\n",
    "       tokens += sub_tok\n",
    "       segment_ids += [1] * len(sub_tok)\n",
    "       if i < len(hds1)-1:\n",
    "           tokens.append(\"[SEP]\")\n",
    "           segment_ids.append(0)\n",
    "       elif i == len(hds1)-1:\n",
    "           tokens.append(\"[SEP]\")\n",
    "           segment_ids.append(1)\n",
    "       else:\n",
    "           raise EnvironmentError\n",
    "\n",
    "\n",
    "   return tokens, segment_ids, header_break\n",
    "\n",
    "def _formatForBert(tokenizer, query_token, header_token, max_seq_length):\n",
    "    #####For each example, tokenize with BERT tokenizer\n",
    "    #Mark each example begining, and separate each header \n",
    "    #so BERT understands the query is a meaningful sequence of words but the headers are not\n",
    "    #concatenate the tokens with breaks but preserve where the headers stop and start\n",
    "    #then pad each example to the max sequence length within each batch\n",
    "    double_tokenized_tokens = []\n",
    "    for (i, token) in enumerate(query_token):\n",
    "            #t_to_tt_idx1.append(\n",
    "            #    len(double_tokenized_tokens))  # all_doc_tokens[ indicate the start position of original 'white-space' tokens.\n",
    "            sub_tokens = tokenizer.tokenize(token)\n",
    "            for sub_token in sub_tokens:\n",
    "            #    tt_to_t_idx1.append(i)\n",
    "                double_tokenized_tokens.append(sub_token)  # all_doc_tokens are further tokenized using WordPiece tokenizer\n",
    "\n",
    "    tokens1, segment_ids1, header_break = generate_inputs(tokenizer, double_tokenized_tokens, header_token)\n",
    "    input_ids1 = tokenizer.convert_tokens_to_ids(tokens1)\n",
    "\n",
    "    # Input masks\n",
    "    # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask1 = [1] * len(input_ids1)\n",
    "\n",
    "        # 3. Zero-pad up to the sequence length.\n",
    "    while len(input_ids1) < max_seq_length:\n",
    "            input_ids1.append(0)\n",
    "            input_mask1.append(0)\n",
    "            segment_ids1.append(0)\n",
    "\n",
    "    assert len(input_ids1) == max_seq_length\n",
    "    assert len(input_mask1) == max_seq_length\n",
    "    assert len(segment_ids1) == max_seq_length\n",
    "    return input_ids1,tokens1,segment_ids1,input_mask1,header_break\n",
    "\n",
    "def formatForBert(train_data,BERT_PT_PATH= \"/DataDrive/master-wikisql/annotated_data/\",bert_type=\"uncased_L-12_H-768_A-12\",max_seq_length=222):\n",
    "    \n",
    "    input_ids = []\n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    input_masks = []\n",
    "    header_breaks = []\n",
    "\n",
    "    bert_config_file = os.path.join(BERT_PT_PATH, f'bert_config_{bert_type}.json')\n",
    "    vocab_file = os.path.join(BERT_PT_PATH, f'vocab_{bert_type}.txt')\n",
    "\n",
    "    bert_config = BertConfig.from_json_file(bert_config_file)\n",
    "    tokenizer = tokenization.FullTokenizer(\n",
    "        vocab_file=vocab_file, do_lower_case=True)\n",
    "    \n",
    "    for train_example in train_data:\n",
    "        query_token = train_example[\"query\"]\n",
    "        header_token = train_example[\"header\"]\n",
    "        input_ids1, tokens1, segment_ids1, input_mask1, header_break = \\\n",
    "                _formatForBert(tokenizer, query_token,header_token,max_seq_length)\n",
    "        input_ids.append(input_ids1)\n",
    "        tokens.append(tokens1)\n",
    "        segment_ids.append(segment_ids1)\n",
    "        input_masks.append(input_mask1)\n",
    "        header_breaks.append(header_break)\n",
    "\n",
    "    #TODO return header breaks and use it too\n",
    "    #bert_inputs = []\n",
    "    return np.array(input_ids), np.array(input_masks), np.array(segment_ids)\n",
    "\n",
    "def getBertOutput(bert_inputs, max_seq_length):\n",
    "    all_input_ids = tf.keras.layers.Input(shape=(max_seq_length,), name = \"input_ids\")\n",
    "    all_input_mask = tf.keras.layers.Input(shape=(max_seq_length,), name = \"input_masks\")\n",
    "    all_segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), name = \"segment_ids\")\n",
    "    bert_output = BertLayer(n_fine_tune_layers=10)(bert_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## READING INPUT, PARSING INTO WHAT WILL BE FED TO THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTrainingData(wikisql_path = \"/DataDrive/master-wikisql/annotated_data/\",sample_size=32,use_reduced_set=False):\n",
    "    train_data, train_table, dev_data, dev_table, _, _ = load_wikisql(wikisql_path,\n",
    "                                                                  use_reduced_set, sample_size, no_w2i=True, no_hs_tok=True)\n",
    "    \n",
    "    return parseToTextLines(train_data,train_table)\n",
    "\n",
    "def parseToTextLines(train_data,train_table):\n",
    "    train_dataframe = pd.DataFrame(train_data)\n",
    "    question_toks = [[x.lower() for x in sublist] for sublist in train_dataframe[\"question_tok\"].tolist()]\n",
    "    sql = train_dataframe[\"sql\"].tolist()\n",
    "    table_ids = train_dataframe[\"table_id\"].tolist()\n",
    "    table_headers = [[x.lower() for x in train_table[tid][\"header\"]] for tid in table_ids]\n",
    "    labels = parseSqlToLabels(sql)\n",
    "    assert len(question_toks)==len(table_headers)\n",
    "    \n",
    "    keras_model_train_data = [{\"query\":question_toks[i],\"header\":table_headers[i]} for i in range(len(question_toks))]\n",
    "    return keras_model_train_data, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wikisql(path_wikisql, toy_model, toy_size, bert=False, no_w2i=False, no_hs_tok=False, aug=False):\n",
    "    # Get data\n",
    "    train_data, train_table = load_wikisql_data(path_wikisql, mode='train', toy_model=toy_model, toy_size=toy_size, no_hs_tok=no_hs_tok, aug=aug)\n",
    "    dev_data, dev_table = load_wikisql_data(path_wikisql, mode='dev', toy_model=toy_model, toy_size=toy_size, no_hs_tok=no_hs_tok)\n",
    "\n",
    "\n",
    "    # Get word vector\n",
    "    if no_w2i:\n",
    "        w2i, wemb = None, None\n",
    "    else:\n",
    "        w2i, wemb = load_w2i_wemb(path_wikisql, bert)\n",
    "\n",
    "\n",
    "    return train_data, train_table, dev_data, dev_table, w2i, wemb\n",
    "\n",
    "\n",
    "def load_wikisql_data(path_wikisql, mode='train', toy_model=False, toy_size=10, no_hs_tok=False, aug=False):\n",
    "    \"\"\" Load training sets\n",
    "    \"\"\"\n",
    "    if aug:\n",
    "        mode = f\"aug.{mode}\"\n",
    "        print('Augmented data is loaded!')\n",
    "\n",
    "    path_sql = os.path.join(path_wikisql, mode+'_tok.jsonl')\n",
    "    if no_hs_tok:\n",
    "        path_table = os.path.join(path_wikisql, mode + '.tables.jsonl')\n",
    "    else:\n",
    "        path_table = os.path.join(path_wikisql, mode+'_tok.tables.jsonl')\n",
    "\n",
    "    data = []\n",
    "    table = {}\n",
    "    with open(path_sql) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if toy_model and idx >= toy_size:\n",
    "                break\n",
    "\n",
    "            t1 = json.loads(line.strip())\n",
    "            data.append(t1)\n",
    "\n",
    "    with open(path_table) as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            if toy_model and idx > toy_size:\n",
    "                break\n",
    "\n",
    "            t1 = json.loads(line.strip())\n",
    "            table[t1['id']] = t1\n",
    "\n",
    "    return data, table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FORMATTING SQL LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_g(sql_i):\n",
    "    \"\"\" for backward compatibility, separated with get_g\"\"\"\n",
    "    g_sc = []\n",
    "    g_sa = []\n",
    "    g_wn = []\n",
    "    g_wc = []\n",
    "    g_wo = []\n",
    "    g_wv = []\n",
    "    for b, psql_i1 in enumerate(sql_i):\n",
    "        g_sc.append( psql_i1[\"sel\"] )\n",
    "        g_sa.append( psql_i1[\"agg\"])\n",
    "\n",
    "        conds = psql_i1['conds']\n",
    "        if not psql_i1[\"agg\"] < 0:\n",
    "            g_wn.append( len( conds ) )\n",
    "            g_wc.append( get_wc1(conds) )\n",
    "            g_wo.append( get_wo1(conds) )\n",
    "            g_wv.append( get_wv1(conds) )\n",
    "        else:\n",
    "            raise EnvironmentError\n",
    "    return g_sc, g_sa, g_wn, g_wc, g_wo, g_wv\n",
    "\n",
    "def padWhereConditions(clause_list):\n",
    "    b = np.zeros([len(clause_list),len(max(clause_list,key = lambda x: len(x)))])\n",
    "    b[:] = -1\n",
    "    for i,j in enumerate(clause_list):\n",
    "        b[i][0:len(j)] = j\n",
    "    return(b)\n",
    "\n",
    "def parseSqlToLabels(sql): \n",
    "    #return sql\n",
    "    #return [random.randint(0, 1) for x in sql]\n",
    "    sc,sa,wn,wc,wo,wv = get_g(sql)\n",
    "    wc,wo = padWhereConditions(wc),padWhereConditions(wo)\n",
    "    return np.array(sc),np.array(sa),np.array(wn),wc,wo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
